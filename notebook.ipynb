{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Alessandro Tofani\n",
    "# Matricola 844145"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "Importo le librerie che userò per l'analisi dati,tra cui pandas, numpy, matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_uuid": "a801cefd9a17a7f619eec88a81aae34f14b97156"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "_uuid": "7bb9aeff202a934258f69ac6ad278b9c5c4d739c"
   },
   "source": [
    "# 1. Data input "
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "Importo i dati in:\n",
    "- train_data = dati per fare allenare il modello.  Contiene l'autore e il subreddit in cui hanno scritto;\n",
    "- target = label dei dati. 0 se sono uomini, 1 se sono donne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "03006e40a92e2e374a494f4abb15d2fefe56d3ff"
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"../input/datamining2020/train_data.csv\", encoding=\"utf8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "Visualizzo le prime righe del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "2600899a23b92befec4ec078402e9c44a65b9897"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Shamus_Aran</td>\n",
       "      <td>mylittlepony</td>\n",
       "      <td>1.388534e+09</td>\n",
       "      <td>I don't think we'd get nearly as much fanficti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Riddance</td>\n",
       "      <td>sex</td>\n",
       "      <td>1.388534e+09</td>\n",
       "      <td>Thanks. I made it up, that's how I got over my...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Secret_Wizard</td>\n",
       "      <td>DragonsDogma</td>\n",
       "      <td>1.388534e+09</td>\n",
       "      <td>Are you sure you aren't confusing Cyclops (the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Penultimatum</td>\n",
       "      <td>malefashionadvice</td>\n",
       "      <td>1.388534e+09</td>\n",
       "      <td>dont do this to me bro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7-SE7EN-7</td>\n",
       "      <td>todayilearned</td>\n",
       "      <td>1.388534e+09</td>\n",
       "      <td>That's what we do when we can't find a mate</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          author          subreddit   created_utc  \\\n",
       "0    Shamus_Aran       mylittlepony  1.388534e+09   \n",
       "1       Riddance                sex  1.388534e+09   \n",
       "2  Secret_Wizard       DragonsDogma  1.388534e+09   \n",
       "3   Penultimatum  malefashionadvice  1.388534e+09   \n",
       "4      7-SE7EN-7      todayilearned  1.388534e+09   \n",
       "\n",
       "                                                body  \n",
       "0  I don't think we'd get nearly as much fanficti...  \n",
       "1  Thanks. I made it up, that's how I got over my...  \n",
       "2  Are you sure you aren't confusing Cyclops (the...  \n",
       "3                             dont do this to me bro  \n",
       "4        That's what we do when we can't find a mate  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "cf29c3c9b927020503dfb6e41d92f9e380915834"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.author.unique().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "Importo il dataset che contiene l'output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "a5bbaa115e83a571c86fcd07277118f0c729a514"
   },
   "outputs": [],
   "source": [
    "target = pd.read_csv(\"../input/datamining2020/train_target.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "6aaa81e4507ae61b34c381fd203e18f34ccfccd7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RedThunder90</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lirkmor</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>In0chi</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ProjectGrudge</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TehTurtleHermit</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            author  gender\n",
       "0     RedThunder90       0\n",
       "1          Lirkmor       1\n",
       "2           In0chi       0\n",
       "3    ProjectGrudge       0\n",
       "4  TehTurtleHermit       0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "_uuid": "787021ec7da4ec1a1b2296c1bc67d8b64f278d43"
   },
   "source": [
    "# 2. Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "Seleziono le features che sono importanti per il modello:\n",
    "- Subreddits alle quale l'autore ha partecipato\n",
    "- Testo del messaggio\n",
    "- Gender dell'autore\n",
    "\n",
    "Estraggo tali informazioni dai rispettivi dataset e le organizzo in matrici. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "4ae83bc0a51e4edcda8903dda7b1ea9971bd8dbc"
   },
   "outputs": [],
   "source": [
    "subreddits = train_data.subreddit.unique()\n",
    "subreddits_map = pd.Series(index=subreddits, data=arange(subreddits.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "Uso la libreria sparse che mi permette di lavorare con matrici sparse, ovvero matrici in cui la maggior parte degli elimenti è nulla. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "fda9ea28517401139a62d9d3c08514a689abb248"
   },
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "from scipy.sparse import hstack"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "Definisco la funzione per estrarre le features. Estraggo il subreddit a cui l'utente ha partecipato. Per fare ciò, la funzione ritorna 1 se l'utente ha partecipato ad un subreddit, 0 altrimenti. \n",
    "\n",
    "Riga = autore\n",
    "\n",
    "Colonna = subreddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "fe7ee5926b14f2e1a338f9e7a8cd4d13d87b13b7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x3468 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_features(group):\n",
    "    group_subreddits = group['subreddit'].values\n",
    "    idxs = subreddits_map[group_subreddits].values\n",
    "    v = sparse.dok_matrix((1, subreddits.shape[0]))\n",
    "    for idx in idxs:\n",
    "        if not np.isnan(idx):\n",
    "            v[0, idx] = 1\n",
    "    return v.tocsr()\n",
    "\n",
    "extract_features(train_data[train_data.author=='RedThunder90'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "21a80468d90131f8021f446dcbcbc8848cdee0aa"
   },
   "outputs": [],
   "source": [
    "features_dict = {}\n",
    "\n",
    "for author, group in train_data.groupby('author'):\n",
    "    features_dict[author] = extract_features(group)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "Per occupare meno spazio in memoria e dato che la matrice contiene tanti zeri, la converto in sparse. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "d15816cd5bc076582b769255b697abd8e028cf43"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5000x3468 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 49152 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = sparse.vstack([features_dict[author] for author in target.author])\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "dc9a85e3dcfab4f35601517fd7076d7f198fb686"
   },
   "outputs": [],
   "source": [
    "y = target.gender"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "Definisco la funzione che estrae il testo del commento dell'autore. Quindi la funzione associa il testo all'autore. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "066e0e0632075a067ff46ce7eb8895c6f1a1caa9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I still prefer to buy foods either grown locally or where animals are treated better, but this definitely has me looking at organic food differently.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_text(group):\n",
    "    group_text = group['body'].values\n",
    "    return \" \".join(map(str,group_text))\n",
    "\n",
    "extract_text(train_data[train_data.author=='RedThunder90'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "96f1df44460515daab04a0c8a6a47ca5ba7eb171"
   },
   "outputs": [],
   "source": [
    "text_dict = {}\n",
    "\n",
    "for author, group in train_data.groupby('author'):\n",
    "    text_dict[author] = extract_text(group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "f9b126773b52e11764ca4dd1ecd23eb3999c8d8e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I still prefer to buy foods either grown locally or where animals are treated better, but this defin'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "author_text = [text_dict[author] for author in target.author]\n",
    "author_text[0][:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "### CountVectorizer\n",
    "Faccio la vectorization di author_text, cioè assegno un token ad ogni parola, e conto i token assegnati. Alla fine manterrò solo le parole che ricorrono di più o che hanno un peso maggiore. Gli elementi della matrice che estraggo sono [autore, parola]. Come prima, anche questa matrice sarà una matrice sparse. \n",
    "\n",
    "Tramite il parametro max_features specifico il massimo numero di parole che il CountVectorizer estrae. Dopo diverse prove, per avere una buona accuracy e consentire che il programma funzionasse correttamente, ho impostato 20000 come limite. \n",
    "\n",
    "Altresì bisogna rimuovere le parole che non portare alcuna informazione utili, come ad esempio le parole brevi. Per farlo uso:\n",
    "- stop_words: non inserisce le stop words della lingua inglese;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "1bf7ebc57de0675737809d580ab996b8e34ac78d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 20000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, ENGLISH_STOP_WORDS\n",
    "\n",
    "pattern ='(?u)\\\\b[A-Za-z]{3,}'\n",
    "\n",
    "stop_words = set(list(ENGLISH_STOP_WORDS) + ['test'])\n",
    "\n",
    "cv = CountVectorizer(token_pattern=pattern, stop_words=stop_words, max_features = 20000)\n",
    "\n",
    "C = cv.fit_transform(author_text)\n",
    "\n",
    "tfidf = TfidfTransformer()\n",
    "\n",
    "X_train = tfidf.fit_transform(C)\n",
    "\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "Si è ottenuto:\n",
    "- X_train: matrice con i valori delle parole usato dagli utenti;\n",
    "- y: matrice con le label, cioè i gender;\n",
    "- X: matrice contenti i subreddit\n",
    "\n",
    "Unisco le due matrici contenenti i dati per formare X_total, che avrà 5000 righe e 23468 colonne, di cui le prime 3468 si riferiscono ai subreddit, mentre le altre alle parole selezionate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5000x23468 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1934913 stored elements in COOrdinate format>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_total = sparse.hstack([X,X_train])\n",
    "X_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_total = X_total.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 3. Model selection"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "## Multinomial naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "Inizialmente ho usato il Multinomial Naive Bayes. Sebbene riuscissi ad ottenere una accuracy in sample superiore a 0.92, non sono riuscito a raggiungere una accuracy out of sample soddisfacente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.naive_bayes import MultinomialNB\n",
    "#from sklearn import model_selection\n",
    "\n",
    "#NB = MultinomialNB()\n",
    "#NB.fit(X_training, y)\n",
    "\n",
    "#alphas = np.logspace(-3, 3, 20)\n",
    "#scores = []\n",
    "#cv1 = model_selection.KFold(n_splits=10, shuffle=True, random_state=0)\n",
    "\n",
    "#for alpha in alphas:\n",
    "#    NB.alpha = alpha\n",
    "#    s = model_selection.cross_val_score(NB, X_training, \n",
    "#                                        y, cv=cv1)\n",
    "#    scores.append(s.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot(alphas, scores)\n",
    "#plt.xscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_alpha = alphas[np.argmax(scores)]\n",
    "#NB = MultinomialNB(alpha=best_alpha)\n",
    "#NB.fit(X_total, y)\n",
    "#print(\"Score: %s\" % NB.score(X_total,y))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "Come secondo modello ho provato la rete neurale, implementandola con keras. Sono riuscito a raggiungere una accuracy in sample decisamente migliore di quella ottenuta con il Naive Bayes, di circa 0.98, impostando 20epochs e 200 di batch size. Prima di arrivare a tali parametri, ho fatto diverse submission per verificare l'accuracy out of sample, e verificare che il modello riuscisse a descrivere bene i dati senza overfittare. \n",
    "\n",
    "Come funzione di attivazione ho optato per la ReLU negli hidden layer, per evitare la saturazione del segnale, mentre nel layer di output ho optato per la sigmoide, avendo così come output la probabilità ell'input di appartenere ad uno dei due gender. \n",
    "\n",
    "Come loss function ho optato per la bynary_crossentropy, dato che il problema è di classificazione. \n",
    "\n",
    "Ho anche provato ad introdurre sia la regolarizzazione L1 che L2, ma non apportava alcun miglioramento al modello. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multilayer_perceptron(nr_hidden=20):\n",
    "    # create model\n",
    "    n_input = 23468\n",
    "    model = Sequential()\n",
    "    model.add(Dense(nr_hidden, input_dim=n_input, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "10/10 [==============================] - 0s 35ms/step - loss: 0.6406 - accuracy: 0.7096\n",
      "Epoch 2/20\n",
      "10/10 [==============================] - 0s 36ms/step - loss: 0.5613 - accuracy: 0.7336\n",
      "Epoch 3/20\n",
      "10/10 [==============================] - 0s 35ms/step - loss: 0.5067 - accuracy: 0.7556\n",
      "Epoch 4/20\n",
      "10/10 [==============================] - 0s 35ms/step - loss: 0.4514 - accuracy: 0.8276\n",
      "Epoch 5/20\n",
      "10/10 [==============================] - 0s 38ms/step - loss: 0.4007 - accuracy: 0.8830\n",
      "Epoch 6/20\n",
      "10/10 [==============================] - 0s 35ms/step - loss: 0.3585 - accuracy: 0.8970\n",
      "Epoch 7/20\n",
      "10/10 [==============================] - 0s 41ms/step - loss: 0.3242 - accuracy: 0.9138\n",
      "Epoch 8/20\n",
      "10/10 [==============================] - 0s 36ms/step - loss: 0.2951 - accuracy: 0.9280\n",
      "Epoch 9/20\n",
      "10/10 [==============================] - 0s 36ms/step - loss: 0.2705 - accuracy: 0.9322\n",
      "Epoch 10/20\n",
      "10/10 [==============================] - 0s 39ms/step - loss: 0.2492 - accuracy: 0.9408\n",
      "Epoch 11/20\n",
      "10/10 [==============================] - 0s 36ms/step - loss: 0.2298 - accuracy: 0.9468\n",
      "Epoch 12/20\n",
      "10/10 [==============================] - 0s 36ms/step - loss: 0.2128 - accuracy: 0.9534\n",
      "Epoch 13/20\n",
      "10/10 [==============================] - 0s 36ms/step - loss: 0.1974 - accuracy: 0.9602\n",
      "Epoch 14/20\n",
      "10/10 [==============================] - 0s 36ms/step - loss: 0.1833 - accuracy: 0.9648\n",
      "Epoch 15/20\n",
      "10/10 [==============================] - 0s 36ms/step - loss: 0.1704 - accuracy: 0.9706\n",
      "Epoch 16/20\n",
      "10/10 [==============================] - 0s 36ms/step - loss: 0.1588 - accuracy: 0.9732\n",
      "Epoch 17/20\n",
      "10/10 [==============================] - 0s 36ms/step - loss: 0.1480 - accuracy: 0.9758\n",
      "Epoch 18/20\n",
      "10/10 [==============================] - 0s 36ms/step - loss: 0.1381 - accuracy: 0.9790\n",
      "Epoch 19/20\n",
      "10/10 [==============================] - 0s 39ms/step - loss: 0.1290 - accuracy: 0.9820\n",
      "Epoch 20/20\n",
      "10/10 [==============================] - 0s 35ms/step - loss: 0.1206 - accuracy: 0.9848\n"
     ]
    }
   ],
   "source": [
    "model = multilayer_perceptron()\n",
    "h = model.fit(X_total, y, epochs=20, batch_size=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "_uuid": "50350b2033979099ec9e5dadd72a05f0b47f2744"
   },
   "source": [
    "# 4. Prepare the solution"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "Ho importato i dati per predirre gli output sul test set. Il test set non contiene i valori del gender degli utenti. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_uuid": "39592879b9f88a56c88ea0f759bdc4f77b4133e3"
   },
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(\"../input/datamining2020/test_data.csv\", encoding=\"utf8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_uuid": "b8e6223088aedf8f4b827b6f7d555d2c22ced142"
   },
   "outputs": [],
   "source": [
    "features_dict = {}\n",
    "\n",
    "for author, group in test_data.groupby('author'):\n",
    "    features_dict[author] = extract_features(group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "_uuid": "74839ff4a639f7e3aac6596e1864332f5ac24d88"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<15000x3468 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 144898 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = sparse.vstack([features_dict[author] for author in test_data.author.unique()])\n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_dict = {}\n",
    "\n",
    "for author, group in test_data.groupby('author'):\n",
    "    text_dict[author] = extract_text(group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I hadn't ever heard of them before joining this subreddit. They're not really a big thing in the US,\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "author_text = [text_dict[author] for author in test_data.author.unique()]\n",
    "author_text[0][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = cv.transform(author_text)\n",
    "\n",
    "X_test1 = tfidf.fit_transform(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_total = sparse.hstack([X_test,X_test1])\n",
    "X_test_total = X_test_total.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15000, 23468)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_total.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "Vado a predirre qual è la probabilità di avere gender 1, usando il modello allenato sul trainig set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = y_pred.reshape(15000,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "_uuid": "3d54c4654aae233bb9aac99a03fb5cd9dfee5915"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ejchristian86</td>\n",
       "      <td>0.998543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ZenDragon</td>\n",
       "      <td>0.000665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>savoytruffle</td>\n",
       "      <td>0.002261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hentercenter</td>\n",
       "      <td>0.009024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rick-o-suave</td>\n",
       "      <td>0.104667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>olivermihoff</td>\n",
       "      <td>0.028586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Cleriesse</td>\n",
       "      <td>0.784628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>murderer_of_death</td>\n",
       "      <td>0.000113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>SpiralSoul</td>\n",
       "      <td>0.015997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>IRideVelociraptors</td>\n",
       "      <td>0.077240</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               author    gender\n",
       "0       ejchristian86  0.998543\n",
       "1           ZenDragon  0.000665\n",
       "2        savoytruffle  0.002261\n",
       "3        hentercenter  0.009024\n",
       "4        rick-o-suave  0.104667\n",
       "5        olivermihoff  0.028586\n",
       "6           Cleriesse  0.784628\n",
       "7   murderer_of_death  0.000113\n",
       "8          SpiralSoul  0.015997\n",
       "9  IRideVelociraptors  0.077240"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solution = pd.DataFrame({\"author\":test_data.author.unique(), \"gender\":y_pred})\n",
    "solution.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "_uuid": "a87ec4dad5a88d40a5562e983fd813cb956eb171"
   },
   "outputs": [],
   "source": [
    "# solution.to_csv(\"solution.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b1068a5b6920198541f469dc5b26e95b99d07252"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
